{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the Libraries"
      ],
      "metadata": {
        "id": "KYB_Pxc5SA1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh70b2XIIIKQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import Vectors\n",
        "#from torchtext.data import Field\\\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "#nltk.download('all')\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, \\\n",
        "    Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cmnZuAsIIdn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the dataset and preprocessing"
      ],
      "metadata": {
        "id": "sgSEleAHSIVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"gigaword\")"
      ],
      "metadata": {
        "id": "623XXMkvIdln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']"
      ],
      "metadata": {
        "id": "GKXnWLC9IdjR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tr = pd.DataFrame(train_data)\n",
        "df_val = pd.DataFrame(val_data)"
      ],
      "metadata": {
        "id": "MmnR-MQBIdgq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_tr, df_val], axis=0)\n",
        "\n",
        "# Resetting the index after concatenation\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Display the concatenated dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wOCOpmVXIdeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "O12GVL1BL7M7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a subset of the data\n",
        "df = df[0:50000]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nc1vCTwqMD6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Remove non-alphabetic characters (Data Cleaning)\n",
        "# def text_strip(column):\n",
        "\n",
        "#     for row in column:\n",
        "#         row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
        "#         row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
        "#         row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove _ if it occurs more than one time consecutively\n",
        "#         row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove - if it occurs more than one time consecutively\n",
        "#         row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove ~ if it occurs more than one time consecutively\n",
        "#         row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove + if it occurs more than one time consecutively\n",
        "#         row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove . if it occurs more than one time consecutively\n",
        "#         row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove the characters - <>()|&©ø\"',;?~*!\n",
        "#         row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!#]\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove mailto:\n",
        "#         row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove \\x9* in text\n",
        "#         row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Replace INC nums to INC_NUM\n",
        "#         row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
        "\n",
        "#         # Replace CM# and CHG# to CM_NUM\n",
        "#         row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
        "\n",
        "#         # Remove punctuations at the end of a word\n",
        "#         row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
        "#         row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
        "#         row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Replace any url to only the domain name\n",
        "#         try:\n",
        "#             url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
        "#             repl_url = url.group(3)\n",
        "#             row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "#         # Remove multiple spaces\n",
        "#         row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         # Remove the single character hanging between any two spaces\n",
        "#         row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
        "\n",
        "#         yield row"
      ],
      "metadata": {
        "id": "RVLr2GQ9IdbM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "cUwJ8iABmxgS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    newString = newString.replace(\" lrb \", \"\").replace(\" rrb \", \"\")\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "metadata": {
        "id": "aEa0oHcSYPT8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = df['document'].apply(text_cleaner)\n",
        "processed_summary = df['summary'].apply(text_cleaner)"
      ],
      "metadata": {
        "id": "N_jlGe4WIdYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['document'] = processed_text\n",
        "df['summary'] = processed_summary"
      ],
      "metadata": {
        "id": "ryIchdI3fSch"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "dyw8qW_8fltS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['summary']].shape"
      ],
      "metadata": {
        "id": "G_IjwVPYo9m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_count = []\n",
        "summary_count = []\n",
        "\n",
        "for sent in df['document']:\n",
        "    text_count.append(len(sent.split()))\n",
        "\n",
        "for sent in df['summary']:\n",
        "    summary_count.append(len(sent.split()))\n",
        "\n",
        "graph_df = pd.DataFrame()\n",
        "\n",
        "graph_df['text'] = text_count\n",
        "graph_df['summary'] = summary_count\n",
        "\n",
        "graph_df.hist(bins = 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jmNkqmi1IdK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_df.head()"
      ],
      "metadata": {
        "id": "6BEtrFN1rFbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how much % of text have 0-30 words\n",
        "cnt = 0\n",
        "for i in df['document']:\n",
        "    if len(i.split()) <= 30:\n",
        "        cnt = cnt + 1\n",
        "print(cnt / len(df['document']))"
      ],
      "metadata": {
        "id": "Wm6ioP0lNhhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how much % of summary have 0-10 words\n",
        "cnt = 0\n",
        "for i in df['summary']:\n",
        "    if len(i.split()) <= 10:\n",
        "        cnt = cnt + 1\n",
        "print(cnt / len(df['summary']))"
      ],
      "metadata": {
        "id": "Cu4QUm2PqWcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
        "max_text_len = 30\n",
        "max_summary_len = 10"
      ],
      "metadata": {
        "id": "-yO04AZRgFTv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "cleaned_text = np.array(df['document'])\n",
        "cleaned_summary= np.array(df['summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "\n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9G6q1hrqgFMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating the lengths\n",
        "\n",
        "text1 =np.array(df['text'])\n",
        "headline1=np.array(df['summary'])\n",
        "\n",
        "\n",
        "for i in range(len(text1)):\n",
        "    if(len(headline1[i].split())>=150):\n",
        "      print(i)"
      ],
      "metadata": {
        "id": "lw5rpVlpgFIH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "vWoBfshjgFGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(\n",
        "    np.array(df[\"text\"]),\n",
        "    np.array(df[\"summary\"]),\n",
        "    test_size=0.3,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xZhCeFxogFDd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_tr))\n",
        "print(len(x_val))"
      ],
      "metadata": {
        "id": "ZxLSO__RgE_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "tHho811Pi9fo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizing and creating word indices"
      ],
      "metadata": {
        "id": "-TIpgjtySTCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(text, summary, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    text=np.array(text)\n",
        "    summary=np.array(summary)\n",
        "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(summary)\n",
        "        output_lang = Lang(text)\n",
        "    else:\n",
        "        input_lang = Lang(text)\n",
        "        output_lang = Lang(summary)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "skRcS-R1i9da"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name,\"--------------------\", input_lang.n_words)\n",
        "    #print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "mdDAgI8Li9WB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(x_tr, y_tr , False)"
      ],
      "metadata": {
        "id": "J3nN-bpyi9PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[5]"
      ],
      "metadata": {
        "id": "EbiiH7xYi9Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "metadata": {
        "id": "pfn2P919i9Jx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "KQtizjuDi9GL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timer = 0\n",
        "for pair in pairs:\n",
        "    # Call tensorsFromPair for each pair\n",
        "    sample_tensors = tensorsFromPair(pair)\n",
        "\n",
        "    # Print the resulting tensors\n",
        "    input_tensor, target_tensor = sample_tensors\n",
        "    print(\"Input Tensor:\")\n",
        "    print(input_tensor)\n",
        "\n",
        "    print(\"\\nTarget Tensor:\")\n",
        "    print(target_tensor)\n",
        "\n",
        "    # Add a separator for better readability\n",
        "    print(\"\\n\" + \"=\" * 30 + \"\\n\")\n",
        "\n",
        "    timer += 1\n",
        "    if timer >= 1:\n",
        "        break"
      ],
      "metadata": {
        "id": "Plj21hfeeRZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentenceFromTensor(lang, tensor):\n",
        "    words = [lang.index2word[index.item()] for index in tensor]\n",
        "    sentence = ' '.join(words)\n",
        "    return sentence\n",
        "\n",
        "# Example tensor\n",
        "example_tensor = torch.tensor([[2],[3],[4],[5],[6],[7],[1]])\n",
        "\n",
        "# Convert tensor to sentence using input_lang\n",
        "original_sentence = sentenceFromTensor(input_lang, example_tensor)\n",
        "\n",
        "# Print the original sentence\n",
        "print(\"Original Sentence:\")\n",
        "print(original_sentence)"
      ],
      "metadata": {
        "id": "lGQjKR86frNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Decoder using Attention and Teacher forcing"
      ],
      "metadata": {
        "id": "2Y5eqV78SfEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(1, 1, self.hidden_size, device=device))"
      ],
      "metadata": {
        "id": "b82Kqq2okGag"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "j69FM5INkGX3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=100"
      ],
      "metadata": {
        "id": "r8LjIio6kGVD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size*2 , self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size*2 , self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.LSTM = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "lNTT4gHTkGQD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        temp = encoder_output[0, 0]\n",
        "        encoder_outputs[ei] = temp\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "      decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "      if decoder_input.item() == EOS_token:\n",
        "             break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "R7ZhUFySkGM_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "vzFW4IDZkGKv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "GBBYeX_tkGHx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showPlot(train_losses, val_losses=None):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='train')\n",
        "\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label='validation')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GOIgA5Gbvedt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for training"
      ],
      "metadata": {
        "id": "zGXaICMDSmwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, val_every=1000):\n",
        "    print(\"Training....\")\n",
        "    start = time.time()\n",
        "    plot_train_losses = []\n",
        "    plot_val_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    val_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        input_length = input_tensor.size(0)\n",
        "        if input_length > 100:\n",
        "            continue\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) Train Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                      iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_train_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_train_losses)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ThKTVqp_g7FA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "VVZubLE6m-CL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=5):\n",
        "    text=list()\n",
        "    headline=list()\n",
        "    pred_headline=list()\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "\n",
        "        if(len(pair[0].split())>=100):\n",
        "          continue\n",
        "        else:\n",
        "          if(i%1000==0):\n",
        "            print(i*100/n,\"% complete\")\n",
        "\n",
        "\n",
        "          text.append(pair[0])\n",
        "\n",
        "          headline.append(pair[1])\n",
        "          output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "          output_sentence = ' '.join(output_words)\n",
        "          pred_headline.append(output_sentence)\n",
        "\n",
        "    return(text,headline,pred_headline)"
      ],
      "metadata": {
        "id": "RiVqx-QPm9_H"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "losses = trainIters(encoder1, attn_decoder1, 10000, print_every=1000)"
      ],
      "metadata": {
        "id": "pAkbKx01m982"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text,headline,pred_headline=evaluateRandomly(encoder1, attn_decoder1,11000)\n",
        "\n",
        "pred_df_LSTM=pd.DataFrame()\n",
        "\n",
        "pred_df_LSTM['text']=text\n",
        "pred_df_LSTM['headline']=headline\n",
        "pred_df_LSTM['pred_headline']=pred_headline"
      ],
      "metadata": {
        "id": "_V6ZNgwy2Tsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,8):\n",
        "  print(\"original Summary>>>\",pred_df_LSTM.iloc[i]['headline'])\n",
        "  print(\"Predicted Summary>>>\",pred_df_LSTM.iloc[i]['pred_headline'])\n",
        "  print('-----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "Bc1jUFgA2Tpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df_LSTM.head()"
      ],
      "metadata": {
        "id": "2IqPcy7kS5Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE Score"
      ],
      "metadata": {
        "id": "AG-8OJoPSyLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rogue(src_trg, pred_trg):\n",
        "\n",
        "  #cut off  token\n",
        "  pred_trg = pred_trg[:-6]\n",
        "\n",
        "\n",
        "  if (len(pred_trg) == 0):\n",
        "    rogue_score = 0.0\n",
        "  else:\n",
        "    s = rouge.get_scores(pred_trg, src_trg, avg= True)\n",
        "    rogue_score = s['rouge-1']['f']\n",
        "\n",
        "  return rogue_score\n"
      ],
      "metadata": {
        "id": "P-JUH_5n2Tm6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_plot_threshold = 0.45\n",
        "\n",
        "def evaluateRandomlyprint_1(encoder, decoder, n=5):\n",
        "    text=list()\n",
        "    headline=list()\n",
        "    pred_headline=list()\n",
        "\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "\n",
        "        if(len(pair[0].split())>=150):\n",
        "          continue\n",
        "        else:\n",
        "\n",
        "\n",
        "          tokenized_input = nltk.word_tokenize(pair[0])\n",
        "          output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "\n",
        "          output_sentence = ' '.join(output_words)\n",
        "          score = calculate_rogue(pair[0], output_sentence)\n",
        "\n",
        "          if score > attn_plot_threshold:\n",
        "            plot_attention(tokenized_input, output_words, attentions, i)"
      ],
      "metadata": {
        "id": "yvK92G-p2TkI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(sentence, predicted_sentence, attention, i):\n",
        "  plt.rcParams.update({'font.size': 18})\n",
        "\n",
        "  fig = plt.figure(figsize=(30,30))\n",
        "\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.matshow(attention, cmap='bone')\n",
        "\n",
        "  ax.set_xticklabels(['']+['']+[t.lower() for t in sentence]+[''], rotation=90)\n",
        "  ax.set_yticklabels(['']+predicted_sentence)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(3))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(3))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LEAiq_Rn2Thu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "\n",
        "evaluateRandomlyprint_1(encoder1, attn_decoder1,15000)\n"
      ],
      "metadata": {
        "id": "TLWn6SDp2TfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "all_text = ' '.join(x_tr)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = all_text.split()\n",
        "\n",
        "# Count the occurrences of each word\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the most common word\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "print(\"Top 10 Most Common Words:\")\n",
        "for word, count in top_10_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "EjKsT5HM2TcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENCODER DECODER USING LSTM"
      ],
      "metadata": {
        "id": "_LELj5rONl--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from time import time\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "\n",
        "# Process text as batches and yield Doc objects in order\n",
        "text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=5000)]\n",
        "\n",
        "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=5000)]"
      ],
      "metadata": {
        "id": "alzj09j6IdV_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dmhtsr5rIdS3",
        "outputId": "a7ab03df-eeeb-42f3-9307-dff48c08460e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sri lanka new foreign minister peiris tuesday urged washington seize business opportunities post war sri lanka rather focus alleged human rights abuses'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wr0j5BZ2IdQQ",
        "outputId": "08d393c3-bbd8-4a8c-9f51-559aa08dd42b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_START_ sri lanka urges focus business rights _END_'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text'] = pd.Series(text)\n",
        "df['cleaned_summary'] = pd.Series(summary)"
      ],
      "metadata": {
        "id": "fvNKoKkUIdNp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wa7IqqgYNneA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph and percentage,  95% of the text pieces fall into the 0-40 category"
      ],
      "metadata": {
        "id": "4cCEoDBAN0Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
        "max_text_len = 25\n",
        "max_summary_len = 11"
      ],
      "metadata": {
        "id": "ENkxcP8JNhft"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "cleaned_text = np.array(df['cleaned_text'])\n",
        "cleaned_summary= np.array(df['cleaned_summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "\n",
        "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
        "\n",
        "post_pre.head()"
      ],
      "metadata": {
        "id": "VrXi2BpsNhdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add sostok and eostok\n",
        "\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
        "        + ' eostok')\n",
        "\n",
        "post_pre.head(2)"
      ],
      "metadata": {
        "id": "Cbpl5oJWNhbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_pre.shape"
      ],
      "metadata": {
        "id": "ilTc20rhO-2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(\n",
        "    np.array(post_pre[\"text\"]),\n",
        "    np.array(post_pre[\"summary\"]),\n",
        "    test_size=0.1,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xKOl_ZbANhZd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr[0]"
      ],
      "metadata": {
        "id": "E0y5CG5Yy_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr[0]"
      ],
      "metadata": {
        "id": "hIjJAvsSytKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text to get the vocab count\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare a tokenizer on training data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "metadata": {
        "id": "iyA4b5LENhXX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tokenizer.word_counts"
      ],
      "metadata": {
        "id": "JsnumcOsy5L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq3sEx2-rIn3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the percentage of occurrence of rare words (say, occurring less than 2 times) in the text.\n",
        "thresh = 2\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
      ],
      "metadata": {
        "id": "v5XiFmEZNhVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ],
      "metadata": {
        "id": "677nCCs9NhS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr.shape"
      ],
      "metadata": {
        "id": "aLKxaulD0ilp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "thresh = 2\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ],
      "metadata": {
        "id": "jORGeEgFNhQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
        "ind = []\n",
        "\n",
        "for i in range(len(y_tr)):\n",
        "    cnt = 0\n",
        "    for j in y_tr[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr = np.delete(y_tr, ind, axis=0)\n",
        "x_tr = np.delete(x_tr, ind, axis=0)"
      ],
      "metadata": {
        "id": "gXTvOSolNhOh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
        "ind = []\n",
        "for i in range(len(y_val)):\n",
        "    cnt = 0\n",
        "    for j in y_val[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "\n",
        "y_val = np.delete(y_val, ind, axis=0)\n",
        "x_val = np.delete(x_val, ind, axis=0)"
      ],
      "metadata": {
        "id": "ioqGCykbNhMb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val[0]"
      ],
      "metadata": {
        "id": "nMFMHOWl3udY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 300\n",
        "embedding_dim = 100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len, ))\n",
        "\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "                    trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "N-nXX35ONhKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "b-dFfhuNNhIQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [x_tr, y_tr[:, :-1]],\n",
        "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "    epochs=50,\n",
        "    callbacks=[es],\n",
        "    batch_size=128,\n",
        "    validation_data=([x_val, y_val[:, :-1]],\n",
        "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
        "                     , 1:]),\n",
        "    )"
      ],
      "metadata": {
        "id": "2Qm-QdksNhGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "LY2qa-n_NhEF"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index"
      ],
      "metadata": {
        "id": "OBICy1EvNhBv"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose generated_indices is a sequence of indices generated by the model\n",
        "generated_indices = model.predict([x_val[0:1], np.zeros((1, max_summary_len-1))])\n",
        "generated_words = [reverse_target_word_index[int(round(idx))]\n",
        "                   for idx in generated_indices.flatten() if int(round(idx)) in reverse_target_word_index]\n",
        "\n",
        "# Print the generated sequence\n",
        "print(\"Generated Sequence: \", ' '.join(generated_words))\n"
      ],
      "metadata": {
        "id": "cPVr77RSmG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "BStX12KdNg_K"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
        "                + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
        "            >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "5ZQ2yzOGP9n4"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert sequence to summary\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
        "            != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "\n",
        "    return newString\n",
        "\n",
        "\n",
        "# To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "\n",
        "    return newString"
      ],
      "metadata": {
        "id": "GlI3a__kP9ly"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 19):\n",
        "    print ('Review:', seq2text(x_tr[i]))\n",
        "    print ('Original summary:', seq2summary(y_tr[i]))\n",
        "    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
        "           max_text_len)))\n",
        "    print ('\\n')"
      ],
      "metadata": {
        "id": "qV2a5nAPP9jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpW9dklcP9hn"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENCODER DECODER WITH LSTM USING GLOVE"
      ],
      "metadata": {
        "id": "-sN7aadUN9Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7O45XyPhvkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have downloaded the GloVe embeddings (e.g., glove.6B.200d.txt)\n",
        "glove_file = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "word2vec_text_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "\n",
        "# Convert GloVe format to Word2Vec format\n",
        "glove2word2vec(glove_file, word2vec_text_file)\n",
        "\n",
        "# Load the Word2Vec format embeddings\n",
        "glove_model = KeyedVectors.load_word2vec_format(word2vec_text_file, binary=False)\n"
      ],
      "metadata": {
        "id": "rPKy75HFvzPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer_glove = Tokenizer(num_words=tot_cnt - cnt)\n",
        "x_tokenizer_glove.fit_on_texts([str(text) for text in x_tr])\n"
      ],
      "metadata": {
        "id": "tKv74rhYP9fj"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text sequences to integer sequences\n",
        "x_tr_seq_glove = x_tokenizer_glove.texts_to_sequences([str(text) for text in x_tr])\n",
        "x_val_seq_glove = x_tokenizer_glove.texts_to_sequences([str(text) for text in x_val])\n"
      ],
      "metadata": {
        "id": "zBnBpdUkP9dd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad zero up to the maximum length\n",
        "x_tr_glove = pad_sequences(x_tr_seq_glove, maxlen=max_text_len, padding='post')\n",
        "x_val_glove = pad_sequences(x_val_seq_glove, maxlen=max_text_len, padding='post')\n"
      ],
      "metadata": {
        "id": "lQms2crLP9aW"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc_glove = x_tokenizer_glove.num_words + 1"
      ],
      "metadata": {
        "id": "H-uy_GJkP9Xx"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a word_index dictionary\n",
        "word_index_glove = x_tokenizer_glove.word_index"
      ],
      "metadata": {
        "id": "AHLJ_MAsP9TD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix\n",
        "embedding_matrix_glove = np.zeros((x_voc_glove, embedding_dim))"
      ],
      "metadata": {
        "id": "fF43VW66P9Q0"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_glove.shape"
      ],
      "metadata": {
        "id": "_uXHJgd6P9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr_glove.shape"
      ],
      "metadata": {
        "id": "fJpXPjgF0o6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in word_index_glove.items():\n",
        "    if word in glove_model:\n",
        "        embedding_matrix_glove[index] = glove_model[word]"
      ],
      "metadata": {
        "id": "yhmw-GCnP9MS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a tokenizer for summary data\n",
        "y_tokenizerglove = Tokenizer(num_words=tot_cnt - cnt)\n",
        "y_tokenizerglove.fit_on_texts([str(text) for text in y_tr])\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "y_tr_seqglove = y_tokenizerglove.texts_to_sequences([str(text) for text in y_tr])\n",
        "y_val_seqglove = y_tokenizerglove.texts_to_sequences([str(text) for text in y_val])\n",
        "\n",
        "# Pad zero up to the maximum length\n",
        "y_tr_glove = pad_sequences(y_tr_seqglove, maxlen=max_summary_len, padding='post')\n",
        "y_val_glove = pad_sequences(y_val_seqglove, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# Size of vocabulary for summary data (+1 for padding token)\n",
        "y_voc_glove = y_tokenizerglove.num_words + 1\n",
        "\n",
        "# Create a word_index dictionary for summary data\n",
        "word_index_glove = y_tokenizerglove.word_index\n",
        "\n",
        "# Create an embedding matrix for summary data\n",
        "yembedding_matrix_glove = np.zeros((y_voc_glove, embedding_dim))\n",
        "\n",
        "for word, index in word_index_glove.items():\n",
        "    if word in glove_model:\n",
        "        embedding_matrix_glove[index] = glove_model[word]"
      ],
      "metadata": {
        "id": "0iRzD_bQ-f4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 300\n",
        "embedding_dim = 100\n",
        "\n",
        "encoder_inputs = Input(shape=(max_text_len, ))\n",
        "enc_emb = Embedding(\n",
        "    x_voc_glove,  # Use the new vocabulary size\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix_glove],\n",
        "    trainable=False\n",
        ")(encoder_inputs)\n",
        "\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9xHYn3oeIIgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "Hb--Vq-5IhUw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [x_tr, y_tr[:, :-1]],\n",
        "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "    epochs=50,\n",
        "    callbacks=[es],\n",
        "    batch_size=128,\n",
        "    validation_data=([x_val, y_val[:, :-1]],\n",
        "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
        "                     , 1:]),\n",
        "    )"
      ],
      "metadata": {
        "id": "kFeF6PfhIhRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "WfdpspH_HpmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
        "                + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
        "            >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "dbNlMHlGHpkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert sequence to summary\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
        "            != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "\n",
        "    return newString\n",
        "\n",
        "\n",
        "# To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "\n",
        "    return newString"
      ],
      "metadata": {
        "id": "XJe6QevnHpfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 19):\n",
        "    print ('Review:', seq2text(x_tr[i]))\n",
        "    print ('Original summary:', seq2summary(y_tr[i]))\n",
        "    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
        "           max_text_len)))\n",
        "    print ('\\n')"
      ],
      "metadata": {
        "id": "muNsVMgmHpcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08-uGkIrHpZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JdOY5gRHpWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLsgLU8a8gyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4QO_0yW8gvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZLu4hzh8gtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0DQFDZv8gq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7bVbE0H8got"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpgoiE3w8gmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxtbuQ238gkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3SuRKcG8ghn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHsrorh08gfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFfjVNYz8gc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9NeE4Qy8gaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}